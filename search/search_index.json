{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to InferAdmin","text":""},{"location":"scope/","title":"Scope for MVP:","text":""},{"location":"scope/#features","title":"Features:","text":"<ul> <li> <p>Local Model storage</p> <ul> <li>Unified location</li> <li>Downloading models (asynchronously)</li> <li>Updating models</li> <li>Deleting models (asynchronously)</li> <li>Storage available</li> </ul> </li> <li> <p>Deploy vLLM docker</p> <ul> <li>Deploy instance (asynchronously)</li> <li>Stop instance (asynchronously)</li> <li>Status of instance (Docker health check)</li> <li>Attach GPUs to instance</li> </ul> </li> <li> <p>vLLM container version control</p> <ul> <li>Dropdown for selecting vLLM container version</li> <li>Checkbox for most recent version</li> <li>Nightly job to pull available version to populate dropdown</li> </ul> </li> <li> <p>Deploy OpenWebUI</p> <ul> <li>Initial config</li> <li>Status</li> <li>Delete</li> <li>Stopping (asynchronously)</li> <li>Status of instance (Docker health check)</li> </ul> </li> <li> <p>GPU state</p> <ul> <li>Vram load</li> <li>Utilization</li> <li>Power consumption</li> </ul> </li> <li> <p>Async operations for IO-bound tasks</p> <ul> <li>Container management</li> <li>Image operations</li> <li>Model downloads</li> <li>Graceful thread pool shutdown</li> </ul> </li> <li> <p>Use health checks for containers</p> </li> <li>Unified local model location</li> </ul>"},{"location":"scope/#config","title":"Config","text":""},{"location":"scope/#llm-specific","title":"LLM Specific","text":"<ul> <li>Ability to set all the args for vLLM when launching instance (pydantic)</li> </ul>"},{"location":"scope/#host-specific","title":"Host Specific","text":"<ul> <li>Unified location of model</li> <li>The port/interface (default to 0.0.0.0) where is deployed on</li> <li>Nvidia docker toolkit installed</li> <li>Refresh interval for gpu state</li> <li>Refresh interval for container state</li> <li>HF token</li> <li>Thread pool sizes for IO and CPU operations</li> </ul>"},{"location":"scope/#operating-data","title":"Operating Data","text":"<ul> <li>What assets are deployed via docker</li> <li>Thread pool utilization metrics</li> </ul>"},{"location":"scope/#architecture","title":"Architecture","text":"<ul> <li>Async operations with specialized thread pools</li> <li>FastApi with asyncio for non-blocking event loop</li> <li>Proper exception handling and propagation</li> <li>Graceful application shutdown</li> </ul>"},{"location":"scope/#stack","title":"Stack","text":"<ul> <li>Deploy InferAdmin in docker</li> <li>InferAdmin interacts with host docker to deploy docker containers for inference/interface</li> <li>FastApi for backend (async mode)</li> <li>Frontend Vue JS + Shadcn</li> <li>YML for config and data, pydantic for representation</li> </ul>"},{"location":"scope/#ideas","title":"Ideas","text":"<ul> <li>Proxying inference requests in front of vLLM to route to correct model<ul> <li>Placeholding /llms endpoint for this functionality</li> </ul> </li> <li>Engines other than vLLM</li> <li>Have multiple storage locations</li> <li>Add analytics for vLLM instances collected from vLLM's prometheus instance</li> <li>Enhanced logging system to replace print statements</li> </ul>"},{"location":"scope/#initial-assumption","title":"Initial assumption","text":"<ul> <li>Nvidia gpus</li> <li>All gpus are same type</li> <li>Interface launches on all exposed via 0.0.0.0</li> </ul>"}]}